<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zikang Shan</title>

    <meta name="author" content="Zikang Shan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <!-- About Me -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zikang Shan
                </p>
                <p>
                  Email: shanzikang [at] stu.pku.edu [dot] cn
                </p>
                <p>
                  I am a Ph.D. student at Peking University, advised by Prof. <a href="http://www.liweiwang-pku.com/">Liwei Wang</a>. 
                  Currently, I am also a research intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a>.
                  Before that, I received my Bachelor's degree from Peking University. 
                  During my undergraduate years, I was honored to be advised by Prof. Liwei Wang and Prof. <a href="https://hughw19.github.io/">He Wang</a>.
                </p>
                <p>
                  Feel free to contact me if you want to discuss or collaborate!
                </p>
                <p style="text-align:center">
                  <a href="https://scholar.google.com/citations?user=qBy4wKcAAAAJ&hl=en">Google Scholar</a> &nbsp;|&nbsp;
                  <a href="https://github.com/zkshan2002/">Github</a>
                </p>
              </td>
              <!-- <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/Profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/JonBarron.jpg" class="hoverZoomLink"></a>
              </td> -->
            </tr>
          </tbody></table>
          <!-- Research -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am interested in <strong>reinforcement learning</strong>, particularly its application in large language model post-training. 
                  I am also tracking advancements in <strong>robotic manipulation</strong>.
                </p>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <!-- RTO -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/rto.png" width="100%" />
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">DPO Meets PPO: Reinforced Token Optimization for RLHF</span>
                <br>
                Han Zhong*, <strong>Zikang Shan*</strong>, Guhao Feng*, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, Liwei Wang
                <br>
                <em>Under review</em>, 2024
                <br>
                  <a href="https://arxiv.org/abs/2404.18922">Paper</a> &nbsp;|&nbsp;
                  <a href="https://github.com/zkshan2002/RTO">Code</a>
                <br>
                  Based on theoretical insights, we propose an RLHF algorithm that is sample efficient and effective.
                <br>
              </td>
            </tr>

            <!-- Unidexgrasp++ -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/unidexgrasp2.jpg" width="100%" />
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">UniDexGrasp++: Improving Universal Dexterous Grasping via Geometry-aware Curriculum Learning and Iterative Generalist-Specialist Learning</span>
                <br>
                  Weikang Wan*, Haoran Geng*, Yun Liu, <strong>Zikang Shan</strong>, Li Yi, Yaodong Yang, and He Wang
                <br>
                <em>ICCV</em>, 2023
                <br>
                <font color='red'>Oral presentation with all top rankings, best paper finalist</font>
                <br>
                  <a href="https://arxiv.org/abs/2304.00464">Paper</a> &nbsp;|&nbsp;
                  <a href="https://pku-epic.github.io/UniDexGrasp++/">Website</a> &nbsp;|&nbsp;
                  <a href="https://github.com/PKU-EPIC/UniDexGrasp2">Code</a>
                <br>
                  We propose an object-agnostic method for learning a universal policy for dexterous object grasping.
                <br>
              </td>
            </tr>

            <!-- Unidexgrasp -->
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/unidexgrasp.png" width="100%" />
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <span class="papertitle">UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</span>
                <br>
                  Yinzhen Xu*, Weikang Wan*, Jialiang Zhang*, Haoran Liu*, <strong>Zikang Shan</strong>, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, Li Yi, and He Wang
                <br>
                <em>CVPR</em>, 2023
                <br>
                  <a href="https://arxiv.org/abs/2303.00938">Paper</a> &nbsp;|&nbsp;
                  <a href="https://pku-epic.github.io/UniDexGrasp/">Website</a> &nbsp;|&nbsp;
                  <a href="https://github.com/PKU-EPIC/UniDexGrasp">Code</a>
                <br>
                  We propose a method for dexterous grasping policy learning, handling diverse objects from realistic observations.
                <br>
              </td>
            </tr>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is adapted from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a> and deployed on <a href="https://pages.github.com/">Github Pages</a>. 
                  Last updated: Feb. 20, 2025
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
